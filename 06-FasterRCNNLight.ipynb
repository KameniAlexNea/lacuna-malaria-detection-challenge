{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"WANDB_PROJECT\"] = \"lacuna_zindi_challenge\"\n",
    "os.environ[\"WANDB_LOG_MODEL\"] = \"false\"\n",
    "os.environ[\"WANDB_WATCH\"] = \"none\"\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"train_hf\"\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models, datasets, ops\n",
    "from torchvision.transforms import v2 as transforms\n",
    "import lightning as L\n",
    "\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "\n",
    "torch.set_float32_matmul_precision('medium') # | 'high'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/eak/learning/zindi_challenge/lacuna_chl/lacuna-malaria-detection-challenge/.venv/lib/python3.9/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from lightning.pytorch.utilities.types import EVAL_DATALOADERS\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "class FacesData(L.LightningDataModule):\n",
    "\ttransform = transforms.Compose([\n",
    "\t\ttransforms.ToTensor(),\n",
    "\t\ttransforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "\t\ttransforms.Resize(size=(800,), max_size=1333),\n",
    "\t])\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef convert_inputs(imgs, annot, device, small_thr=0.001):\n",
    "\t\t\"\"\"Conver dataset item to accepted target struture.\"\"\"\n",
    "\t\timages, targets = [], []\n",
    "\t\tfor img, annot in zip(imgs, annot):\n",
    "\t\t\tbbox = annot['bbox']\n",
    "\t\t\tsmall = (bbox[:, 2] * bbox[:, 3]) <= (img.size[1] * img.size[0] * small_thr)\n",
    "\t\t\tboxes = ops.box_convert(bbox[~small], in_fmt='xywh', out_fmt='xyxy')\n",
    "\t\t\toutput_dict = FacesData.transform({\"image\": img, \"boxes\": boxes})\n",
    "\t\t\timages.append(output_dict['image'].to(device))\n",
    "\t\t\ttargets.append({\n",
    "\t\t\t\t'boxes': output_dict['boxes'].to(device),\n",
    "\t\t\t\t'labels': torch.ones(len(boxes), dtype=int, device=device)\n",
    "\t\t\t})\n",
    "\t\treturn images, targets\n",
    "\t\n",
    "\t@staticmethod\n",
    "\tdef _collate_fn(batch):\n",
    "\t\t\"\"\"Define a collate function to handle batches.\"\"\"\n",
    "\t\treturn tuple(zip(*batch))\n",
    "\n",
    "\tdef train_dataloader(self):\n",
    "\t\ttrain_dataset = datasets.WIDERFace(root='./data', split='train', download=True)\n",
    "\t\treturn DataLoader(\n",
    "\t\t\ttrain_dataset, batch_size=batch_size, shuffle=True, num_workers=4, collate_fn=self._collate_fn\n",
    "\t\t)\n",
    "\t\n",
    "\tdef val_dataloader(self):\n",
    "\t\ttrain_dataset = datasets.WIDERFace(root='./data', split='val', download=True)\n",
    "\t\treturn DataLoader(\n",
    "\t\t\ttrain_dataset, batch_size=batch_size, shuffle=True, num_workers=4, collate_fn=self._collate_fn\n",
    "\t\t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "data = FacesData()\n",
    "\n",
    "train = data.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = next(iter(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1024x668>,\n",
       "  <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1024x528>,\n",
       "  <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1024x1365>,\n",
       "  <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1024x1024>,\n",
       "  <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1024x630>,\n",
       "  <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1024x1415>,\n",
       "  <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1024x694>,\n",
       "  <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1024x1754>,\n",
       "  <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1024x1532>,\n",
       "  <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1024x1324>,\n",
       "  <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1024x800>,\n",
       "  <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1024x768>,\n",
       "  <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1024x769>,\n",
       "  <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1024x690>,\n",
       "  <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1024x683>,\n",
       "  <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1024x768>),\n",
       " ({'bbox': tensor([[556, 200,  24,  35],\n",
       "           [309, 206,  26,  27],\n",
       "           [244, 276,  20,  24]]),\n",
       "   'blur': tensor([2, 2, 2]),\n",
       "   'expression': tensor([0, 0, 0]),\n",
       "   'illumination': tensor([0, 0, 0]),\n",
       "   'occlusion': tensor([0, 0, 0]),\n",
       "   'pose': tensor([0, 0, 0]),\n",
       "   'invalid': tensor([0, 0, 0])},\n",
       "  {'bbox': tensor([[154, 134,  37,  53],\n",
       "           [496,  21,  26,  31],\n",
       "           [584,  99,  26,  33],\n",
       "           [402,  73,  28,  41],\n",
       "           [598, 334,  34,  42],\n",
       "           [497, 326,  34,  40],\n",
       "           [412, 287,  34,  49],\n",
       "           [834,  69,  85, 119],\n",
       "           [670, 361,  10,  14],\n",
       "           [640, 361,  10,  12]]),\n",
       "   'blur': tensor([1, 1, 1, 1, 1, 1, 1, 0, 2, 2]),\n",
       "   'expression': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "   'illumination': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "   'occlusion': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "   'pose': tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 1]),\n",
       "   'invalid': tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])},\n",
       "  {'bbox': tensor([[304, 112, 315, 461]]),\n",
       "   'blur': tensor([0]),\n",
       "   'expression': tensor([0]),\n",
       "   'illumination': tensor([0]),\n",
       "   'occlusion': tensor([0]),\n",
       "   'pose': tensor([0]),\n",
       "   'invalid': tensor([0])},\n",
       "  {'bbox': tensor([[438, 106, 128, 172],\n",
       "           [724, 118, 130, 170],\n",
       "           [136, 118, 126, 164]]),\n",
       "   'blur': tensor([0, 0, 0]),\n",
       "   'expression': tensor([0, 0, 0]),\n",
       "   'illumination': tensor([0, 0, 0]),\n",
       "   'occlusion': tensor([0, 0, 0]),\n",
       "   'pose': tensor([0, 0, 0]),\n",
       "   'invalid': tensor([0, 0, 0])},\n",
       "  {'bbox': tensor([[380,  30,  68,  78]]),\n",
       "   'blur': tensor([1]),\n",
       "   'expression': tensor([0]),\n",
       "   'illumination': tensor([0]),\n",
       "   'occlusion': tensor([0]),\n",
       "   'pose': tensor([0]),\n",
       "   'invalid': tensor([0])},\n",
       "  {'bbox': tensor([[384, 295, 229, 306]]),\n",
       "   'blur': tensor([0]),\n",
       "   'expression': tensor([0]),\n",
       "   'illumination': tensor([0]),\n",
       "   'occlusion': tensor([0]),\n",
       "   'pose': tensor([2]),\n",
       "   'invalid': tensor([0])},\n",
       "  {'bbox': tensor([[946, 260,  20,  28],\n",
       "           [902, 253,  13,  29],\n",
       "           [843, 273,  16,  25],\n",
       "           [749, 249,  18,  25],\n",
       "           [662, 257,  11,  24],\n",
       "           [624, 244,  11,  24],\n",
       "           [608, 238,  17,  25],\n",
       "           [567, 255,  11,  21],\n",
       "           [523, 253,  11,  24],\n",
       "           [492, 250,  15,  23],\n",
       "           [410, 236,  11,  22],\n",
       "           [379, 252,  16,  22],\n",
       "           [330, 248,  16,  22],\n",
       "           [274, 252,  10,  24],\n",
       "           [240, 265,  16,  19],\n",
       "           [181, 247,  15,  22],\n",
       "           [161, 257,   7,   9],\n",
       "           [109, 257,  12,  17],\n",
       "           [ 39, 254,  17,  19],\n",
       "           [715, 270,  15,  20],\n",
       "           [758, 246,  17,  27]]),\n",
       "   'blur': tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n",
       "   'expression': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "   'illumination': tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1]),\n",
       "   'occlusion': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "   'pose': tensor([0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 2, 1, 0, 2, 2]),\n",
       "   'invalid': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])},\n",
       "  {'bbox': tensor([[469, 182, 267, 353]]),\n",
       "   'blur': tensor([0]),\n",
       "   'expression': tensor([0]),\n",
       "   'illumination': tensor([1]),\n",
       "   'occlusion': tensor([0]),\n",
       "   'pose': tensor([0]),\n",
       "   'invalid': tensor([0])},\n",
       "  {'bbox': tensor([[536, 147, 108, 153],\n",
       "           [527, 395,  96, 123]]),\n",
       "   'blur': tensor([0, 0]),\n",
       "   'expression': tensor([0, 0]),\n",
       "   'illumination': tensor([0, 0]),\n",
       "   'occlusion': tensor([0, 0]),\n",
       "   'pose': tensor([0, 0]),\n",
       "   'invalid': tensor([0, 1])},\n",
       "  {'bbox': tensor([[315,   5, 649, 796]]),\n",
       "   'blur': tensor([0]),\n",
       "   'expression': tensor([0]),\n",
       "   'illumination': tensor([0]),\n",
       "   'occlusion': tensor([0]),\n",
       "   'pose': tensor([0]),\n",
       "   'invalid': tensor([0])},\n",
       "  {'bbox': tensor([[193, 156,  76,  68],\n",
       "           [478, 121,  87,  93],\n",
       "           [957, 260,  52,  72],\n",
       "           [813, 245,  68,  92],\n",
       "           [605, 189,  45,  59],\n",
       "           [669, 290,  52,  61],\n",
       "           [169, 297,  53,  54],\n",
       "           [899, 277,  45,  59],\n",
       "           [472, 256,  49,  69]]),\n",
       "   'blur': tensor([1, 0, 2, 2, 2, 2, 2, 2, 2]),\n",
       "   'expression': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "   'illumination': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "   'occlusion': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "   'pose': tensor([0, 1, 0, 1, 0, 2, 0, 1, 2]),\n",
       "   'invalid': tensor([0, 0, 0, 0, 0, 1, 0, 0, 0])},\n",
       "  {'bbox': tensor([[748, 166,  37,  45]]),\n",
       "   'blur': tensor([1]),\n",
       "   'expression': tensor([0]),\n",
       "   'illumination': tensor([0]),\n",
       "   'occlusion': tensor([0]),\n",
       "   'pose': tensor([0]),\n",
       "   'invalid': tensor([0])},\n",
       "  {'bbox': tensor([[829, 187,  56,  55],\n",
       "           [655, 157,  48,  53],\n",
       "           [493, 154,  51,  56],\n",
       "           [326, 175,  49,  64],\n",
       "           [167, 195,  54,  64],\n",
       "           [736, 219,   5,   7],\n",
       "           [759, 198,   6,   7],\n",
       "           [807, 209,   5,   7],\n",
       "           [817, 207,   5,   7],\n",
       "           [932, 207,   4,   8],\n",
       "           [905, 213,   5,   5],\n",
       "           [893, 210,   4,   7],\n",
       "           [785, 232,   8,  11]]),\n",
       "   'blur': tensor([0, 1, 1, 1, 1, 2, 2, 2, 2, 2, 0, 2, 2]),\n",
       "   'expression': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "   'illumination': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "   'occlusion': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]),\n",
       "   'pose': tensor([0, 0, 0, 0, 0, 0, 2, 0, 1, 2, 0, 1, 1]),\n",
       "   'invalid': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])},\n",
       "  {'bbox': tensor([[386, 174,  86, 110],\n",
       "           [824, 202,  82, 112]]),\n",
       "   'blur': tensor([0, 0]),\n",
       "   'expression': tensor([0, 0]),\n",
       "   'illumination': tensor([0, 0]),\n",
       "   'occlusion': tensor([0, 0]),\n",
       "   'pose': tensor([0, 0]),\n",
       "   'invalid': tensor([0, 0])},\n",
       "  {'bbox': tensor([[849, 142,  50,  60],\n",
       "           [843,  50,  38,  52],\n",
       "           [610,  70,  35,  40],\n",
       "           [397, 173,  46,  49],\n",
       "           [ 50,  52,  30,  33],\n",
       "           [585, 160,  16,  14]]),\n",
       "   'blur': tensor([1, 1, 1, 1, 1, 2]),\n",
       "   'expression': tensor([0, 0, 0, 0, 0, 0]),\n",
       "   'illumination': tensor([0, 0, 0, 0, 0, 0]),\n",
       "   'occlusion': tensor([0, 0, 0, 0, 0, 0]),\n",
       "   'pose': tensor([1, 1, 1, 1, 1, 1]),\n",
       "   'invalid': tensor([0, 0, 0, 0, 0, 0])},\n",
       "  {'bbox': tensor([[272, 102, 194, 244]]),\n",
       "   'blur': tensor([0]),\n",
       "   'expression': tensor([0]),\n",
       "   'illumination': tensor([0]),\n",
       "   'occlusion': tensor([0]),\n",
       "   'pose': tensor([0]),\n",
       "   'invalid': tensor([0])}))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, targets = FacesData.convert_inputs(example[0], example[1], device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'boxes': tensor([[556, 200, 580, 235],\n",
       "          [309, 206, 335, 233]], device='cuda:0'),\n",
       "  'labels': tensor([1, 1], device='cuda:0')},\n",
       " {'boxes': tensor([[154, 134, 191, 187],\n",
       "          [496,  21, 522,  52],\n",
       "          [584,  99, 610, 132],\n",
       "          [402,  73, 430, 114],\n",
       "          [598, 334, 632, 376],\n",
       "          [497, 326, 531, 366],\n",
       "          [412, 287, 446, 336],\n",
       "          [834,  69, 919, 188]], device='cuda:0'),\n",
       "  'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')},\n",
       " {'boxes': tensor([[304, 112, 619, 573]], device='cuda:0'),\n",
       "  'labels': tensor([1], device='cuda:0')},\n",
       " {'boxes': tensor([[438, 106, 566, 278],\n",
       "          [724, 118, 854, 288],\n",
       "          [136, 118, 262, 282]], device='cuda:0'),\n",
       "  'labels': tensor([1, 1, 1], device='cuda:0')},\n",
       " {'boxes': tensor([[380,  30, 448, 108]], device='cuda:0'),\n",
       "  'labels': tensor([1], device='cuda:0')},\n",
       " {'boxes': tensor([[384, 295, 613, 601]], device='cuda:0'),\n",
       "  'labels': tensor([1], device='cuda:0')},\n",
       " {'boxes': tensor([], device='cuda:0', size=(0, 4), dtype=torch.int64),\n",
       "  'labels': tensor([], device='cuda:0', dtype=torch.int64)},\n",
       " {'boxes': tensor([[469, 182, 736, 535]], device='cuda:0'),\n",
       "  'labels': tensor([1], device='cuda:0')},\n",
       " {'boxes': tensor([[536, 147, 644, 300],\n",
       "          [527, 395, 623, 518]], device='cuda:0'),\n",
       "  'labels': tensor([1, 1], device='cuda:0')},\n",
       " {'boxes': tensor([[315,   5, 964, 801]], device='cuda:0'),\n",
       "  'labels': tensor([1], device='cuda:0')},\n",
       " {'boxes': tensor([[ 193,  156,  269,  224],\n",
       "          [ 478,  121,  565,  214],\n",
       "          [ 957,  260, 1009,  332],\n",
       "          [ 813,  245,  881,  337],\n",
       "          [ 605,  189,  650,  248],\n",
       "          [ 669,  290,  721,  351],\n",
       "          [ 169,  297,  222,  351],\n",
       "          [ 899,  277,  944,  336],\n",
       "          [ 472,  256,  521,  325]], device='cuda:0'),\n",
       "  'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')},\n",
       " {'boxes': tensor([[748, 166, 785, 211]], device='cuda:0'),\n",
       "  'labels': tensor([1], device='cuda:0')},\n",
       " {'boxes': tensor([[829, 187, 885, 242],\n",
       "          [655, 157, 703, 210],\n",
       "          [493, 154, 544, 210],\n",
       "          [326, 175, 375, 239],\n",
       "          [167, 195, 221, 259]], device='cuda:0'),\n",
       "  'labels': tensor([1, 1, 1, 1, 1], device='cuda:0')},\n",
       " {'boxes': tensor([[386, 174, 472, 284],\n",
       "          [824, 202, 906, 314]], device='cuda:0'),\n",
       "  'labels': tensor([1, 1], device='cuda:0')},\n",
       " {'boxes': tensor([[849, 142, 899, 202],\n",
       "          [843,  50, 881, 102],\n",
       "          [610,  70, 645, 110],\n",
       "          [397, 173, 443, 222],\n",
       "          [ 50,  52,  80,  85]], device='cuda:0'),\n",
       "  'labels': tensor([1, 1, 1, 1, 1], device='cuda:0')},\n",
       " {'boxes': tensor([[272, 102, 466, 346]], device='cuda:0'),\n",
       "  'labels': tensor([1], device='cuda:0')}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[ 1.9235,  1.9364,  1.9507,  ...,  2.1975,  2.1975,  2.1975],\n",
       "          [ 1.9364,  1.9396,  1.9507,  ...,  2.1975,  2.1975,  2.1975],\n",
       "          [ 1.9507,  1.9507,  1.9549,  ...,  2.1975,  2.1975,  2.1975],\n",
       "          ...,\n",
       "          [-1.1225, -1.2310, -1.3774,  ...,  1.2534,  1.2576,  1.2576],\n",
       "          [-0.9965, -1.1125, -1.2639,  ...,  1.1729,  1.1829,  1.1829],\n",
       "          [-0.9192, -1.0352, -1.1941,  ...,  1.1600,  1.1700,  1.1700]],\n",
       " \n",
       "         [[ 2.0784,  2.0916,  2.1062,  ...,  2.3761,  2.3761,  2.3761],\n",
       "          [ 2.0916,  2.0949,  2.1062,  ...,  2.3761,  2.3761,  2.3761],\n",
       "          [ 2.1062,  2.1062,  2.1105,  ...,  2.3761,  2.3761,  2.3761],\n",
       "          ...,\n",
       "          [-1.0706, -1.1815, -1.3106,  ...,  1.3584,  1.3626,  1.3626],\n",
       "          [-0.9418, -1.0604, -1.1946,  ...,  1.2760,  1.2863,  1.2863],\n",
       "          [-0.8627, -0.9814, -1.1233,  ...,  1.2628,  1.2731,  1.2731]],\n",
       " \n",
       "         [[ 2.3786,  2.3917,  2.4062,  ...,  2.5877,  2.5877,  2.5877],\n",
       "          [ 2.3917,  2.3949,  2.4062,  ...,  2.5877,  2.5877,  2.5877],\n",
       "          [ 2.4062,  2.4062,  2.4105,  ...,  2.5877,  2.5877,  2.5877],\n",
       "          ...,\n",
       "          [-0.9308, -1.0412, -1.1697,  ...,  1.4382,  1.4219,  1.4219],\n",
       "          [-0.8025, -0.9206, -1.0542,  ...,  1.3562,  1.3459,  1.3459],\n",
       "          [-0.7238, -0.8419, -0.9832,  ...,  1.3431,  1.3328,  1.3328]]],\n",
       "        device='cuda:0'),\n",
       " tensor([[[1.9920, 2.0144, 1.9543,  ..., 2.0027, 1.9354, 1.9578],\n",
       "          [1.9809, 2.0251, 2.0283,  ..., 2.0640, 2.0054, 1.9913],\n",
       "          [1.9316, 2.0157, 2.0374,  ..., 2.1102, 2.0665, 2.0236],\n",
       "          ...,\n",
       "          [1.8975, 1.9227, 1.9327,  ..., 2.2098, 2.2041, 2.1606],\n",
       "          [1.7858, 1.9058, 1.9456,  ..., 2.2395, 2.2194, 2.1640],\n",
       "          [1.7523, 1.9087, 1.9632,  ..., 2.2489, 2.2310, 2.1975]],\n",
       " \n",
       "         [[2.4286, 2.4286, 2.3623,  ..., 2.4184, 2.4172, 2.4286],\n",
       "          [2.4171, 2.4246, 2.4056,  ..., 2.4250, 2.4246, 2.4286],\n",
       "          [2.3594, 2.4045, 2.3913,  ..., 2.4131, 2.4286, 2.4286],\n",
       "          ...,\n",
       "          [2.3982, 2.3988, 2.3832,  ..., 2.3681, 2.4044, 2.3779],\n",
       "          [2.3700, 2.4082, 2.4142,  ..., 2.3720, 2.3888, 2.3471],\n",
       "          [2.3585, 2.4042, 2.4286,  ..., 2.3816, 2.3928, 2.3585]],\n",
       " \n",
       "         [[2.6400, 2.6400, 2.5960,  ..., 2.6400, 2.6400, 2.6400],\n",
       "          [2.6400, 2.6400, 2.6247,  ..., 2.6400, 2.6400, 2.6400],\n",
       "          [2.6033, 2.6272, 2.6122,  ..., 2.6215, 2.6400, 2.6400],\n",
       "          ...,\n",
       "          [2.5492, 2.5683, 2.5384,  ..., 2.6118, 2.6365, 2.6299],\n",
       "          [2.4771, 2.5759, 2.5751,  ..., 2.6231, 2.6360, 2.6286],\n",
       "          [2.4657, 2.5794, 2.5960,  ..., 2.6327, 2.6400, 2.6400]]],\n",
       "        device='cuda:0'),\n",
       " tensor([[[ 0.1633,  0.1675,  0.0389,  ...,  1.4908,  1.4804,  1.4783],\n",
       "          [ 0.2426,  0.1967,  0.0635,  ...,  1.4954,  1.4902,  1.4804],\n",
       "          [ 0.2767,  0.2190,  0.1350,  ...,  1.5022,  1.4954,  1.4908],\n",
       "          ...,\n",
       "          [-0.6356, -0.6024, -0.7409,  ...,  1.1660,  1.1747,  1.2122],\n",
       "          [-0.7232, -0.6327, -0.6961,  ...,  1.1760,  1.1795,  1.2053],\n",
       "          [-0.6495, -0.6775, -0.5280,  ...,  1.1808,  1.1795,  1.1997]],\n",
       " \n",
       "         [[-0.0909, -0.0840, -0.2127,  ...,  1.7936,  1.7829,  1.7808],\n",
       "          [-0.0179, -0.0571, -0.2033,  ...,  1.7983,  1.7930,  1.7829],\n",
       "          [ 0.0224, -0.0348, -0.1201,  ...,  1.8052,  1.7983,  1.7936],\n",
       "          ...,\n",
       "          [-1.0281, -1.0589, -1.2965,  ...,  1.3566,  1.3557,  1.3862],\n",
       "          [-1.1529, -1.1255, -1.2762,  ...,  1.3667,  1.3606,  1.3792],\n",
       "          [-1.1203, -1.2040, -1.1487,  ...,  1.3716,  1.3606,  1.3734]],\n",
       " \n",
       "         [[-0.5634, -0.5735, -0.7342,  ...,  1.7464,  1.7358,  1.7337],\n",
       "          [-0.5929, -0.6432, -0.7940,  ...,  1.7511,  1.7458,  1.7358],\n",
       "          [-0.7146, -0.7715, -0.8496,  ...,  1.7580,  1.7511,  1.7464],\n",
       "          ...,\n",
       "          [-1.0313, -1.0051, -1.1463,  ...,  1.2067,  1.2347,  1.2886],\n",
       "          [-1.1484, -1.0623, -1.1185,  ...,  1.2168,  1.2396,  1.2816],\n",
       "          [-1.0976, -1.1241, -0.9759,  ...,  1.2218,  1.2396,  1.2758]]],\n",
       "        device='cuda:0'),\n",
       " tensor([[[1.8550, 1.8550, 1.8443,  ..., 1.8443, 1.8550, 1.8550],\n",
       "          [1.8550, 1.8550, 1.8443,  ..., 1.8443, 1.8550, 1.8550],\n",
       "          [1.8550, 1.8550, 1.8443,  ..., 1.8443, 1.8550, 1.8550],\n",
       "          ...,\n",
       "          [1.8550, 1.8550, 1.8443,  ..., 1.8443, 1.8550, 1.8550],\n",
       "          [1.8550, 1.8550, 1.8443,  ..., 1.8443, 1.8550, 1.8550],\n",
       "          [1.8550, 1.8550, 1.8443,  ..., 1.8443, 1.8550, 1.8550]],\n",
       " \n",
       "         [[2.0259, 2.0259, 2.0149,  ..., 2.0149, 2.0259, 2.0259],\n",
       "          [2.0259, 2.0259, 2.0149,  ..., 2.0149, 2.0259, 2.0259],\n",
       "          [2.0259, 2.0259, 2.0149,  ..., 2.0149, 2.0259, 2.0259],\n",
       "          ...,\n",
       "          [2.0259, 2.0259, 2.0149,  ..., 2.0149, 2.0259, 2.0259],\n",
       "          [2.0259, 2.0259, 2.0149,  ..., 2.0149, 2.0259, 2.0259],\n",
       "          [2.0259, 2.0259, 2.0149,  ..., 2.0149, 2.0259, 2.0259]],\n",
       " \n",
       "         [[2.2391, 2.2391, 2.2282,  ..., 2.2282, 2.2391, 2.2391],\n",
       "          [2.2391, 2.2391, 2.2282,  ..., 2.2282, 2.2391, 2.2391],\n",
       "          [2.2391, 2.2391, 2.2282,  ..., 2.2282, 2.2391, 2.2391],\n",
       "          ...,\n",
       "          [2.2391, 2.2391, 2.2282,  ..., 2.2282, 2.2391, 2.2391],\n",
       "          [2.2391, 2.2391, 2.2282,  ..., 2.2282, 2.2391, 2.2391],\n",
       "          [2.2391, 2.2391, 2.2282,  ..., 2.2282, 2.2391, 2.2391]]],\n",
       "        device='cuda:0'),\n",
       " tensor([[[2.2318, 2.2318, 2.2318,  ..., 2.2489, 2.2489, 2.2489],\n",
       "          [2.2318, 2.2318, 2.2318,  ..., 2.2489, 2.2489, 2.2489],\n",
       "          [2.2318, 2.2318, 2.2318,  ..., 2.2489, 2.2489, 2.2489],\n",
       "          ...,\n",
       "          [0.5728, 0.8566, 1.2596,  ..., 2.2489, 2.2489, 2.2489],\n",
       "          [0.5831, 0.9550, 1.4386,  ..., 2.2489, 2.2489, 2.2489],\n",
       "          [0.5364, 0.9799, 1.5407,  ..., 2.2489, 2.2489, 2.2489]],\n",
       " \n",
       "         [[2.4111, 2.4111, 2.4111,  ..., 2.4286, 2.4286, 2.4286],\n",
       "          [2.4111, 2.4111, 2.4111,  ..., 2.4286, 2.4286, 2.4286],\n",
       "          [2.4111, 2.4111, 2.4111,  ..., 2.4286, 2.4286, 2.4286],\n",
       "          ...,\n",
       "          [0.5050, 0.7951, 1.2071,  ..., 2.4286, 2.4286, 2.4286],\n",
       "          [0.5155, 0.8957, 1.3901,  ..., 2.4286, 2.4286, 2.4286],\n",
       "          [0.4678, 0.9212, 1.4945,  ..., 2.4286, 2.4286, 2.4286]],\n",
       " \n",
       "         [[2.6226, 2.6226, 2.6226,  ..., 2.6400, 2.6400, 2.6400],\n",
       "          [2.6226, 2.6226, 2.6226,  ..., 2.6400, 2.6400, 2.6400],\n",
       "          [2.6226, 2.6226, 2.6226,  ..., 2.6400, 2.6400, 2.6400],\n",
       "          ...,\n",
       "          [0.4461, 0.7349, 1.1451,  ..., 2.6400, 2.6400, 2.6400],\n",
       "          [0.4566, 0.8351, 1.3273,  ..., 2.6400, 2.6400, 2.6400],\n",
       "          [0.4091, 0.8605, 1.4312,  ..., 2.6400, 2.6400, 2.6400]]],\n",
       "        device='cuda:0'),\n",
       " tensor([[[-1.9638, -1.9638, -1.9638,  ..., -1.9638, -1.9638, -1.9638],\n",
       "          [-1.9638, -1.9638, -1.9638,  ..., -1.9638, -1.9638, -1.9638],\n",
       "          [-1.9638, -1.9638, -1.9638,  ..., -1.9638, -1.9638, -1.9638],\n",
       "          ...,\n",
       "          [-2.0494, -2.0494, -2.0494,  ..., -1.6898, -1.6898, -1.6898],\n",
       "          [-2.0494, -2.0494, -2.0494,  ..., -1.6898, -1.6898, -1.6898],\n",
       "          [-2.0494, -2.0494, -2.0494,  ..., -1.6898, -1.6898, -1.6898]],\n",
       " \n",
       "         [[-1.8081, -1.8081, -1.8081,  ..., -1.8081, -1.8081, -1.8081],\n",
       "          [-1.8081, -1.8081, -1.8081,  ..., -1.8081, -1.8081, -1.8081],\n",
       "          [-1.8081, -1.8081, -1.8081,  ..., -1.8081, -1.8081, -1.8081],\n",
       "          ...,\n",
       "          [-1.8957, -1.8957, -1.8957,  ..., -1.5455, -1.5455, -1.5455],\n",
       "          [-1.8957, -1.8957, -1.8957,  ..., -1.5455, -1.5455, -1.5455],\n",
       "          [-1.8957, -1.8957, -1.8957,  ..., -1.5455, -1.5455, -1.5455]],\n",
       " \n",
       "         [[-1.5604, -1.5604, -1.5604,  ..., -1.5604, -1.5604, -1.5604],\n",
       "          [-1.5604, -1.5604, -1.5604,  ..., -1.5604, -1.5604, -1.5604],\n",
       "          [-1.5604, -1.5604, -1.5604,  ..., -1.5604, -1.5604, -1.5604],\n",
       "          ...,\n",
       "          [-1.6476, -1.6476, -1.6476,  ..., -1.4384, -1.4384, -1.4384],\n",
       "          [-1.6476, -1.6476, -1.6476,  ..., -1.4384, -1.4384, -1.4384],\n",
       "          [-1.6476, -1.6476, -1.6476,  ..., -1.4384, -1.4384, -1.4384]]],\n",
       "        device='cuda:0'),\n",
       " tensor([[[-1.9467, -1.9467, -1.9467,  ..., -0.6736, -0.6999, -0.7137],\n",
       "          [-1.9604, -1.9604, -1.9604,  ..., -0.6828, -0.6972, -0.7000],\n",
       "          [-1.9752, -1.9752, -1.9752,  ..., -0.6928, -0.6943, -0.6851],\n",
       "          ...,\n",
       "          [-0.3880, -0.2920, -0.2110,  ...,  0.0981, -0.0666, -0.0385],\n",
       "          [-0.5183, -0.3865, -0.2623,  ..., -0.0695, -0.4232, -0.6207],\n",
       "          [-0.6281, -0.4633, -0.2850,  ..., -0.1470, -0.6068, -0.9363]],\n",
       " \n",
       "         [[-1.8606, -1.8606, -1.8606,  ..., -0.6117, -0.6386, -0.6527],\n",
       "          [-1.8747, -1.8747, -1.8747,  ..., -0.6211, -0.6358, -0.6386],\n",
       "          [-1.8899, -1.8899, -1.8899,  ..., -0.6313, -0.6328, -0.6234],\n",
       "          ...,\n",
       "          [-0.2964, -0.1888, -0.1116,  ...,  0.1444, -0.0460,  0.0202],\n",
       "          [-0.4179, -0.2832, -0.1562,  ..., -0.0629, -0.4377, -0.6171],\n",
       "          [-0.5301, -0.3617, -0.1794,  ..., -0.1608, -0.6309, -0.9678]],\n",
       " \n",
       "         [[-1.6302, -1.6302, -1.6302,  ..., -0.1951, -0.2218, -0.2358],\n",
       "          [-1.6441, -1.6441, -1.6441,  ..., -0.2044, -0.2191, -0.2219],\n",
       "          [-1.6592, -1.6592, -1.6592,  ..., -0.2145, -0.2161, -0.2067],\n",
       "          ...,\n",
       "          [-1.1534, -1.0789, -0.9828,  ..., -0.9646, -1.1459, -1.0940],\n",
       "          [-1.3127, -1.1899, -1.0568,  ..., -1.1569, -1.5139, -1.6648],\n",
       "          [-1.4384, -1.2708, -1.0893,  ..., -1.2451, -1.6647, -1.8044]]],\n",
       "        device='cuda:0'),\n",
       " tensor([[[-2.0665, -2.0582, -2.0449,  ..., -1.9908, -2.0278, -2.0601],\n",
       "          [-2.0582, -2.0500, -2.0339,  ..., -1.9866, -2.0157, -2.0507],\n",
       "          [-2.0372, -2.0339, -2.0219,  ..., -1.9738, -2.0027, -2.0399],\n",
       "          ...,\n",
       "          [-1.4904, -1.4575, -1.4206,  ..., -1.4932, -1.4932, -1.4932],\n",
       "          [-1.4832, -1.4943, -1.5690,  ..., -1.2584, -1.2584, -1.2584],\n",
       "          [-1.5578, -1.5367, -1.4656,  ..., -1.1589, -1.1589, -1.1589]],\n",
       " \n",
       "         [[-1.8957, -1.8872, -1.8735,  ..., -1.9058, -1.9436, -1.9766],\n",
       "          [-1.8872, -1.8787, -1.8623,  ..., -1.9015, -1.9312, -1.9670],\n",
       "          [-1.8656, -1.8623, -1.8500,  ..., -1.8884, -1.9180, -1.9560],\n",
       "          ...,\n",
       "          [-1.1316, -1.1149, -1.0974,  ..., -1.3795, -1.3795, -1.3795],\n",
       "          [-1.1242, -1.1525, -1.2492,  ..., -1.1395, -1.1395, -1.1395],\n",
       "          [-1.2005, -1.1959, -1.1434,  ..., -1.0378, -1.0378, -1.0378]],\n",
       " \n",
       "         [[-1.7696, -1.7611, -1.7476,  ..., -1.6751, -1.7128, -1.7456],\n",
       "          [-1.7612, -1.7527, -1.7364,  ..., -1.6708, -1.7004, -1.7360],\n",
       "          [-1.7397, -1.7364, -1.7241,  ..., -1.6578, -1.6872, -1.7251],\n",
       "          ...,\n",
       "          [-0.8172, -0.7921, -0.7679,  ..., -1.2383, -1.2383, -1.2383],\n",
       "          [-0.8099, -0.8296, -0.9189,  ..., -0.9993, -0.9993, -0.9993],\n",
       "          [-0.8858, -0.8728, -0.8136,  ..., -0.8981, -0.8981, -0.8981]]],\n",
       "        device='cuda:0'),\n",
       " tensor([[[ 1.8414,  1.8741,  1.8437,  ...,  1.8795,  1.8053,  1.7465],\n",
       "          [ 1.8536,  1.8672,  1.8731,  ...,  1.9221,  1.8011,  1.7390],\n",
       "          [ 1.9091,  1.9216,  1.8551,  ...,  1.9212,  1.8448,  1.7558],\n",
       "          ...,\n",
       "          [ 1.8768,  1.8816,  1.8761,  ...,  1.8483,  1.8739,  1.9097],\n",
       "          [ 1.8768,  1.8858,  1.8864,  ...,  1.8704,  1.8730,  1.8855],\n",
       "          [ 1.8768,  1.8893,  1.8971,  ...,  1.8954,  1.8778,  1.8630]],\n",
       " \n",
       "         [[ 2.4182,  2.4144,  2.3918,  ...,  2.3481,  2.4243,  2.4286],\n",
       "          [ 2.3838,  2.3841,  2.3695,  ...,  2.3534,  2.4129,  2.4286],\n",
       "          [ 2.3900,  2.3853,  2.2836,  ...,  2.2872,  2.4030,  2.4286],\n",
       "          ...,\n",
       "          [ 2.3953,  2.3697,  2.3320,  ...,  2.3287,  2.4030,  2.4286],\n",
       "          [ 2.4089,  2.3953,  2.3758,  ...,  2.4029,  2.4215,  2.4286],\n",
       "          [ 2.4145,  2.4103,  2.3936,  ...,  2.4286,  2.4286,  2.4286]],\n",
       " \n",
       "         [[-1.8044, -1.8044, -1.7897,  ..., -1.5825, -1.7192, -1.8010],\n",
       "          [-1.7689, -1.7328, -1.6232,  ..., -1.4718, -1.7078, -1.8023],\n",
       "          [-1.6097, -1.5036, -1.3837,  ..., -1.3608, -1.6195, -1.7820],\n",
       "          ...,\n",
       "          [-1.7850, -1.6088, -1.2694,  ..., -1.5565, -1.7387, -1.8044],\n",
       "          [-1.8023, -1.7329, -1.5375,  ..., -1.6976, -1.7904, -1.8044],\n",
       "          [-1.8044, -1.7882, -1.6966,  ..., -1.7787, -1.8044, -1.8044]]],\n",
       "        device='cuda:0'),\n",
       " tensor([[[-2.1179, -2.1179, -2.1179,  ...,  0.8051,  0.6786,  0.6163],\n",
       "          [-2.1179, -2.1179, -2.1179,  ...,  0.7327,  0.6621,  0.6487],\n",
       "          [-2.1179, -2.1179, -2.1179,  ...,  0.6073,  0.6506,  0.7157],\n",
       "          ...,\n",
       "          [-2.1179, -2.1179, -2.1179,  ...,  1.8785,  1.8893,  1.8893],\n",
       "          [-2.1179, -2.1179, -2.1179,  ...,  1.8785,  1.8893,  1.8893],\n",
       "          [-2.1179, -2.1179, -2.1179,  ...,  1.8785,  1.8893,  1.8893]],\n",
       " \n",
       "         [[-2.0357, -2.0357, -2.0357,  ...,  0.2833,  0.2376,  0.2074],\n",
       "          [-2.0357, -2.0357, -2.0357,  ...,  0.2114,  0.2050,  0.2248],\n",
       "          [-2.0357, -2.0357, -2.0357,  ...,  0.0761,  0.1740,  0.2740],\n",
       "          ...,\n",
       "          [-2.0357, -2.0357, -2.0357,  ...,  2.0674,  2.0784,  2.0784],\n",
       "          [-2.0357, -2.0357, -2.0357,  ...,  2.0674,  2.0784,  2.0784],\n",
       "          [-2.0357, -2.0357, -2.0357,  ...,  2.0674,  2.0784,  2.0784]],\n",
       " \n",
       "         [[-1.8044, -1.8044, -1.8044,  ..., -0.0818, -0.1128, -0.1116],\n",
       "          [-1.8044, -1.8044, -1.8044,  ..., -0.1744, -0.1375, -0.0922],\n",
       "          [-1.8044, -1.8044, -1.8044,  ..., -0.3056, -0.1708, -0.0465],\n",
       "          ...,\n",
       "          [-1.7696, -1.7696, -1.7696,  ...,  2.3153,  2.3263,  2.3263],\n",
       "          [-1.7696, -1.7696, -1.7696,  ...,  2.3153,  2.3263,  2.3263],\n",
       "          [-1.7696, -1.7696, -1.7696,  ...,  2.3153,  2.3263,  2.3263]]],\n",
       "        device='cuda:0'),\n",
       " tensor([[[ 1.6324,  1.6324,  1.6324,  ...,  1.6153,  1.6153,  1.6153],\n",
       "          [ 1.6324,  1.6324,  1.6324,  ...,  1.6153,  1.6153,  1.6153],\n",
       "          [ 1.6324,  1.6324,  1.6324,  ...,  1.6153,  1.6153,  1.6153],\n",
       "          ...,\n",
       "          [-1.9467, -1.9467, -1.9295,  ..., -1.7069, -1.5185, -1.2445],\n",
       "          [-1.9467, -1.9467, -1.9295,  ..., -1.6384, -1.3302, -0.9705],\n",
       "          [-1.9467, -1.9467, -1.9295,  ..., -1.5528, -1.1760, -0.7650]],\n",
       " \n",
       "         [[ 1.8859,  1.8859,  1.8859,  ...,  1.8333,  1.8333,  1.8333],\n",
       "          [ 1.8859,  1.8859,  1.8859,  ...,  1.8333,  1.8333,  1.8333],\n",
       "          [ 1.8859,  1.8859,  1.8859,  ...,  1.8333,  1.8333,  1.8333],\n",
       "          ...,\n",
       "          [-2.0007, -2.0007, -1.9832,  ..., -1.6331, -1.4405, -1.1604],\n",
       "          [-2.0007, -2.0007, -1.9832,  ..., -1.5630, -1.2479, -0.8803],\n",
       "          [-2.0007, -2.0007, -1.9832,  ..., -1.4755, -1.0903, -0.6702]],\n",
       " \n",
       "         [[ 2.1694,  2.1694,  2.1694,  ...,  2.1346,  2.1346,  2.1346],\n",
       "          [ 2.1694,  2.1694,  2.1694,  ...,  2.1346,  2.1346,  2.1346],\n",
       "          [ 2.1694,  2.1694,  2.1694,  ...,  2.1346,  2.1346,  2.1346],\n",
       "          ...,\n",
       "          [-1.8044, -1.8044, -1.7870,  ..., -1.4733, -1.2816, -1.0027],\n",
       "          [-1.8044, -1.8044, -1.7870,  ..., -1.4036, -1.0898, -0.7238],\n",
       "          [-1.8044, -1.8044, -1.7870,  ..., -1.3164, -0.9330, -0.5147]]],\n",
       "        device='cuda:0'),\n",
       " tensor([[[-1.6213, -1.4118, -1.1208,  ..., -2.0822, -1.8810, -1.3815],\n",
       "          [-1.7179, -1.4933, -1.1722,  ..., -2.0838, -1.8981, -1.4137],\n",
       "          [-1.7857, -1.5311, -1.1786,  ..., -2.0885, -1.9445, -1.4466],\n",
       "          ...,\n",
       "          [-0.6709, -0.7095, -0.7135,  ..., -0.7274, -0.6945, -0.6623],\n",
       "          [-0.7479, -0.6938, -0.7105,  ..., -0.7274, -0.6945, -0.6623],\n",
       "          [-0.7479, -0.6029, -0.6864,  ..., -0.7274, -0.6945, -0.6623]],\n",
       " \n",
       "         [[-1.5280, -1.3139, -1.0163,  ..., -2.0185, -1.8286, -1.3179],\n",
       "          [-1.6268, -1.3971, -1.0689,  ..., -2.0201, -1.8460, -1.3508],\n",
       "          [-1.6961, -1.4358, -1.0754,  ..., -2.0248, -1.8935, -1.3845],\n",
       "          ...,\n",
       "          [-0.5564, -0.5959, -0.6000,  ..., -0.6142, -0.5806, -0.5476],\n",
       "          [-0.6352, -0.5798, -0.5968,  ..., -0.6142, -0.5806, -0.5476],\n",
       "          [-0.6352, -0.4869, -0.5723,  ..., -0.6142, -0.5806, -0.5476]],\n",
       " \n",
       "         [[-1.2990, -1.0858, -0.7896,  ..., -1.7507, -1.5459, -1.0376],\n",
       "          [-1.3973, -1.1687, -0.8420,  ..., -1.7523, -1.5633, -1.0703],\n",
       "          [-1.4663, -1.2072, -0.8484,  ..., -1.7571, -1.6105, -1.1038],\n",
       "          ...,\n",
       "          [-0.3317, -0.3710, -0.3751,  ..., -0.3892, -0.3558, -0.3230],\n",
       "          [-0.4101, -0.3550, -0.3720,  ..., -0.3892, -0.3558, -0.3230],\n",
       "          [-0.4101, -0.2625, -0.3475,  ..., -0.3892, -0.3558, -0.3230]]],\n",
       "        device='cuda:0'),\n",
       " tensor([[[-0.1999, -0.2161, -0.2171,  ..., -2.0665, -2.0665, -2.0665],\n",
       "          [-0.2322, -0.2331, -0.2332,  ..., -2.0665, -2.0665, -2.0665],\n",
       "          [-0.2496, -0.2496, -0.2496,  ..., -2.0665, -2.0665, -2.0665],\n",
       "          ...,\n",
       "          [-1.9003, -1.8987, -1.8582,  ..., -2.0837, -2.0837, -2.0837],\n",
       "          [-1.9447, -1.9295, -1.9440,  ..., -2.0846, -2.0847, -2.0856],\n",
       "          [-1.9124, -1.9124, -1.9279,  ..., -2.1008, -2.1018, -2.1179]],\n",
       " \n",
       "         [[-0.0924, -0.1089, -0.1099,  ..., -2.0182, -2.0182, -2.0182],\n",
       "          [-0.1254, -0.1264, -0.1264,  ..., -2.0182, -2.0182, -2.0182],\n",
       "          [-0.1433, -0.1433, -0.1433,  ..., -2.0182, -2.0182, -2.0182],\n",
       "          ...,\n",
       "          [-1.7398, -1.6919, -1.6920,  ..., -2.0007, -2.0007, -2.0007],\n",
       "          [-1.7556, -1.7245, -1.7533,  ..., -2.0017, -2.0018, -2.0027],\n",
       "          [-1.7556, -1.7556, -1.7714,  ..., -2.0182, -2.0192, -2.0357]],\n",
       " \n",
       "         [[-0.3578, -0.3742, -0.3753,  ..., -1.7696, -1.7696, -1.7696],\n",
       "          [-0.3907, -0.3916, -0.3917,  ..., -1.7696, -1.7696, -1.7696],\n",
       "          [-0.4084, -0.4084, -0.4084,  ..., -1.7696, -1.7696, -1.7696],\n",
       "          ...,\n",
       "          [-1.3007, -1.2859, -1.2565,  ..., -1.7696, -1.7696, -1.7696],\n",
       "          [-1.3174, -1.3193, -1.3185,  ..., -1.7706, -1.7707, -1.7716],\n",
       "          [-1.3339, -1.3667, -1.3530,  ..., -1.7870, -1.7880, -1.8044]]],\n",
       "        device='cuda:0'),\n",
       " tensor([[[-1.1932, -1.2340, -1.2783,  ..., -1.3463, -1.2576, -1.1760],\n",
       "          [-1.1932, -1.2340, -1.2694,  ..., -1.3603, -1.3068, -1.2576],\n",
       "          [-1.1707, -1.2026, -1.2407,  ..., -1.2743, -1.2568, -1.2338],\n",
       "          ...,\n",
       "          [-1.4500, -1.4364, -1.4252,  ..., -1.1531, -1.2396, -1.3280],\n",
       "          [-1.4500, -1.4364, -1.4104,  ..., -1.1983, -1.2298, -1.1614],\n",
       "          [-1.4500, -1.4364, -1.4104,  ..., -1.1707, -1.1614, -1.0390]],\n",
       " \n",
       "         [[-0.5826, -0.6243, -0.6696,  ..., -0.8617, -0.7711, -0.6877],\n",
       "          [-0.5826, -0.6243, -0.6605,  ..., -0.8760, -0.8214, -0.7711],\n",
       "          [-0.5941, -0.6267, -0.6657,  ..., -0.7652, -0.7472, -0.7238],\n",
       "          ...,\n",
       "          [-1.3179, -1.3040, -1.2925,  ..., -0.8923, -1.0513, -1.1921],\n",
       "          [-1.3179, -1.3040, -1.2774,  ..., -0.9130, -1.0009, -0.9420],\n",
       "          [-1.3179, -1.3040, -1.2774,  ..., -0.8753, -0.9003, -0.7752]],\n",
       " \n",
       "         [[-0.8458, -0.8874, -0.9325,  ..., -1.0365, -0.9463, -0.8633],\n",
       "          [-0.8458, -0.8874, -0.9234,  ..., -1.0508, -0.9964, -0.9463],\n",
       "          [-0.8458, -0.8783, -0.9021,  ..., -0.9519, -0.9340, -0.9107],\n",
       "          ...,\n",
       "          [-1.1421, -1.1283, -1.1168,  ..., -0.8110, -0.9219, -1.0114],\n",
       "          [-1.1421, -1.1283, -1.1018,  ..., -0.8406, -0.8805, -0.7889],\n",
       "          [-1.1421, -1.1283, -1.1018,  ..., -0.7936, -0.7612, -0.6367]]],\n",
       "        device='cuda:0'),\n",
       " tensor([[[-1.5014, -1.5148, -1.4968,  ..., -1.0157, -1.0048, -1.0048],\n",
       "          [-1.4880, -1.5014, -1.4834,  ..., -1.0607, -1.0583, -1.0583],\n",
       "          [-1.4843, -1.4892, -1.4757,  ..., -1.0407, -1.0431, -1.0516],\n",
       "          ...,\n",
       "          [ 1.7974,  1.7755,  1.7408,  ...,  1.7276,  1.7707,  1.7974],\n",
       "          [ 1.7790,  1.7686,  1.7487,  ...,  1.7329,  1.7581,  1.7790],\n",
       "          [ 1.7523,  1.7523,  1.7523,  ...,  1.7414,  1.7523,  1.7523]],\n",
       " \n",
       "         [[-1.4755, -1.4892, -1.4708,  ..., -1.2018, -1.2129, -1.2129],\n",
       "          [-1.4618, -1.4755, -1.4571,  ..., -1.2478, -1.2676, -1.2676],\n",
       "          [-1.4580, -1.4630, -1.4492,  ..., -1.2496, -1.2742, -1.2829],\n",
       "          ...,\n",
       "          [ 1.9670,  1.9446,  1.9091,  ...,  1.8956,  1.9396,  1.9670],\n",
       "          [ 1.9482,  1.9375,  1.9172,  ...,  1.9011,  1.9269,  1.9482],\n",
       "          [ 1.9209,  1.9209,  1.9209,  ...,  1.9097,  1.9209,  1.9209]],\n",
       " \n",
       "         [[-1.2641, -1.2778, -1.2816,  ..., -1.1944, -1.1944, -1.1944],\n",
       "          [-1.2505, -1.2641, -1.2680,  ..., -1.2402, -1.2488, -1.2488],\n",
       "          [-1.2467, -1.2517, -1.2601,  ..., -1.2169, -1.2444, -1.2531],\n",
       "          ...,\n",
       "          [ 2.2153,  2.1931,  2.1577,  ...,  2.1443,  2.1881,  2.2153],\n",
       "          [ 2.1966,  2.1860,  2.1657,  ...,  2.1497,  2.1754,  2.1966],\n",
       "          [ 2.1694,  2.1694,  2.1694,  ...,  2.1583,  2.1694,  2.1694]]],\n",
       "        device='cuda:0'),\n",
       " tensor([[[-0.2513, -0.2996, -0.3644,  ..., -2.0837, -2.0837, -2.0837],\n",
       "          [-0.2513, -0.2996, -0.3644,  ..., -2.0837, -2.0837, -2.0837],\n",
       "          [-0.2359, -0.2842, -0.3629,  ..., -2.0837, -2.0837, -2.0837],\n",
       "          ...,\n",
       "          [-2.0323, -2.0323, -2.0323,  ..., -2.0494, -2.0494, -2.0494],\n",
       "          [-2.0323, -2.0323, -2.0323,  ..., -2.0494, -2.0494, -2.0494],\n",
       "          [-2.0323, -2.0323, -2.0323,  ..., -2.0494, -2.0494, -2.0494]],\n",
       " \n",
       "         [[ 0.5028,  0.4534,  0.3714,  ..., -1.9482, -1.9482, -1.9482],\n",
       "          [ 0.5028,  0.4534,  0.3714,  ..., -1.9482, -1.9482, -1.9482],\n",
       "          [ 0.4870,  0.4376,  0.3698,  ..., -1.9482, -1.9482, -1.9482],\n",
       "          ...,\n",
       "          [-1.8606, -1.8606, -1.8606,  ..., -1.9657, -1.9657, -1.9657],\n",
       "          [-1.8606, -1.8606, -1.8606,  ..., -1.9657, -1.9657, -1.9657],\n",
       "          [-1.8606, -1.8606, -1.8606,  ..., -1.9657, -1.9657, -1.9657]],\n",
       " \n",
       "         [[ 1.8034,  1.7542,  1.7040,  ..., -1.6302, -1.6302, -1.6302],\n",
       "          [ 1.8034,  1.7542,  1.7040,  ..., -1.6302, -1.6302, -1.6302],\n",
       "          [ 1.8034,  1.7542,  1.7040,  ..., -1.6302, -1.6302, -1.6302],\n",
       "          ...,\n",
       "          [-1.5779, -1.5779, -1.5779,  ..., -1.6999, -1.6999, -1.6999],\n",
       "          [-1.5779, -1.5779, -1.5779,  ..., -1.6999, -1.6999, -1.6999],\n",
       "          [-1.5779, -1.5779, -1.5779,  ..., -1.6999, -1.6999, -1.6999]]],\n",
       "        device='cuda:0')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.detection.fasterrcnn_mobilenet_v3_large_fpn(weights=\"DEFAULT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda()\n",
    "with torch.no_grad():\n",
    "    preds = model(images, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss_classifier': tensor(0.2817, device='cuda:0'),\n",
       " 'loss_box_reg': tensor(0.0409, device='cuda:0'),\n",
       " 'loss_objectness': tensor(0.6015, device='cuda:0'),\n",
       " 'loss_rpn_box_reg': tensor(0.0535, device='cuda:0')}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    preds = model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'boxes': tensor([[142.0757, 366.5052, 167.6238, 448.6838],\n",
       "          [765.8087, 563.9625, 888.4448, 684.9437]], device='cuda:0'),\n",
       "  'labels': tensor([10, 85], device='cuda:0'),\n",
       "  'scores': tensor([0.0667, 0.0543], device='cuda:0')},\n",
       " {'boxes': tensor([], device='cuda:0', size=(0, 4)),\n",
       "  'labels': tensor([], device='cuda:0', dtype=torch.int64),\n",
       "  'scores': tensor([], device='cuda:0')},\n",
       " {'boxes': tensor([[   0.0000,  173.3589,  767.8617, 1066.0000],\n",
       "          [  64.2902,  381.8713,  759.6177, 1050.0862]], device='cuda:0'),\n",
       "  'labels': tensor([ 1, 28], device='cuda:0'),\n",
       "  'scores': tensor([0.6858, 0.1167], device='cuda:0')},\n",
       " {'boxes': tensor([], device='cuda:0', size=(0, 4)),\n",
       "  'labels': tensor([], device='cuda:0', dtype=torch.int64),\n",
       "  'scores': tensor([], device='cuda:0')},\n",
       " {'boxes': tensor([[301.3986,  46.7504, 663.4830, 561.3995],\n",
       "          [289.9357,  37.9297, 662.0736, 330.5159]], device='cuda:0'),\n",
       "  'labels': tensor([10,  1], device='cuda:0'),\n",
       "  'scores': tensor([0.4412, 0.0919], device='cuda:0')},\n",
       " {'boxes': tensor([[ 188.2439,  416.6639,  587.9362, 1085.8033]], device='cuda:0'),\n",
       "  'labels': tensor([72], device='cuda:0'),\n",
       "  'scores': tensor([0.0726], device='cuda:0')},\n",
       " {'boxes': tensor([], device='cuda:0', size=(0, 4)),\n",
       "  'labels': tensor([], device='cuda:0', dtype=torch.int64),\n",
       "  'scores': tensor([], device='cuda:0')},\n",
       " {'boxes': tensor([], device='cuda:0', size=(0, 4)),\n",
       "  'labels': tensor([], device='cuda:0', dtype=torch.int64),\n",
       "  'scores': tensor([], device='cuda:0')},\n",
       " {'boxes': tensor([[ 101.0308,  124.9521,  674.7812, 1147.2880],\n",
       "          [ 101.4098,  219.0327,  674.0026,  757.6527],\n",
       "          [ 307.0983,  209.7207,  630.1804, 1171.6162],\n",
       "          [ 187.0118,  267.5020,  381.0914, 1115.6614],\n",
       "          [ 108.4698,  224.1668,  692.0553,  765.1917],\n",
       "          [  77.5500,  145.7093,  675.1276, 1162.6157],\n",
       "          [ 128.8963,  207.7858,  659.9764, 1103.3824],\n",
       "          [  77.3736,  579.8813,  279.8950,  690.1050],\n",
       "          [  52.1898,  165.9831,  704.5979, 1134.5594]], device='cuda:0'),\n",
       "  'labels': tensor([ 1, 72, 32, 32, 15, 62, 10, 15, 72], device='cuda:0'),\n",
       "  'scores': tensor([0.8918, 0.2033, 0.1666, 0.1480, 0.1317, 0.1153, 0.0834, 0.0614, 0.0516],\n",
       "         device='cuda:0')},\n",
       " {'boxes': tensor([], device='cuda:0', size=(0, 4)),\n",
       "  'labels': tensor([], device='cuda:0', dtype=torch.int64),\n",
       "  'scores': tensor([], device='cuda:0')},\n",
       " {'boxes': tensor([[318.1805,  32.5514, 945.6408, 767.1408]], device='cuda:0'),\n",
       "  'labels': tensor([72], device='cuda:0'),\n",
       "  'scores': tensor([0.0791], device='cuda:0')},\n",
       " {'boxes': tensor([], device='cuda:0', size=(0, 4)),\n",
       "  'labels': tensor([], device='cuda:0', dtype=torch.int64),\n",
       "  'scores': tensor([], device='cuda:0')},\n",
       " {'boxes': tensor([], device='cuda:0', size=(0, 4)),\n",
       "  'labels': tensor([], device='cuda:0', dtype=torch.int64),\n",
       "  'scores': tensor([], device='cuda:0')},\n",
       " {'boxes': tensor([[ 933.1062,  170.4125, 1148.3616,  522.7443]], device='cuda:0'),\n",
       "  'labels': tensor([10], device='cuda:0'),\n",
       "  'scores': tensor([0.1219], device='cuda:0')},\n",
       " {'boxes': tensor([], device='cuda:0', size=(0, 4)),\n",
       "  'labels': tensor([], device='cuda:0', dtype=torch.int64),\n",
       "  'scores': tensor([], device='cuda:0')},\n",
       " {'boxes': tensor([], device='cuda:0', size=(0, 4)),\n",
       "  'labels': tensor([], device='cuda:0', dtype=torch.int64),\n",
       "  'scores': tensor([], device='cuda:0')}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a pretrained Faster R-CNN model from torchvision and modify it\n",
    "class FaceDetectionModel(L.LightningModule):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.model = models.detection.fasterrcnn_mobilenet_v3_large_fpn(weights=\"DEFAULT\")\n",
    "\n",
    "\tdef forward(self, images, targets=None):\n",
    "\t\treturn self.model(images, targets)\n",
    "\n",
    "\tdef training_step(self, batch, batch_idx):\n",
    "\t\timgs, annot = batch\n",
    "\t\timages, targets = FacesData.convert_inputs(imgs, annot, device=self.device)\n",
    "\t\tloss_dict = self.model(images, targets)\n",
    "\t\tlosses = sum(loss for loss in loss_dict.values())\n",
    "\t\tself.log(\"loss\", losses)\n",
    "\t\tself.log_dict(loss_dict)\n",
    "\t\treturn losses\n",
    "\t\n",
    "\t# def validation_step(self, batch, batch_idx):\n",
    "\t# \timgs, annot = batch\n",
    "\t# \timages, targets = FacesData.convert_inputs(imgs, annot, device=self.device)\n",
    "\t# \tloss_dict = self.model(images, targets)\n",
    "\t# \tlosses = sum(loss for loss in loss_dict.values())\n",
    "\t# \tself.log(\"loss\", losses)\n",
    "\t# \tself.log_dict(loss_dict)\n",
    "\t# \treturn losses\n",
    "\n",
    "\tdef configure_optimizers(self):\n",
    "\t\treturn optim.SGD(self.parameters(), lr=0.001, momentum=0.9, weight_decay=0.0005)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/data/home/eak/learning/zindi_challenge/lacuna_chl/lacuna-malaria-detection-challenge/.venv/lib/python3.9/site-packages/lightning/pytorch/trainer/configuration_validator.py:68: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n",
      "You are using a CUDA device ('NVIDIA RTX A6000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name  | Type       | Params | Mode \n",
      "---------------------------------------------\n",
      "0 | model | FasterRCNN | 19.4 M | train\n",
      "---------------------------------------------\n",
      "19.3 M    Trainable params\n",
      "58.9 K    Non-trainable params\n",
      "19.4 M    Total params\n",
      "77.545    Total estimated model params size (MB)\n",
      "280       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Epoch 1:   4%|         | 30/805 [00:21<09:22,  1.38it/s, v_num=0] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/learning/zindi_challenge/lacuna_chl/lacuna-malaria-detection-challenge/.venv/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "File \u001b[0;32m~/learning/zindi_challenge/lacuna_chl/lacuna-malaria-detection-challenge/.venv/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py:574\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    568\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    570\u001b[0m     ckpt_path,\n\u001b[1;32m    571\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    572\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    573\u001b[0m )\n\u001b[0;32m--> 574\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n",
      "File \u001b[0;32m~/learning/zindi_challenge/lacuna_chl/lacuna-malaria-detection-challenge/.venv/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py:981\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 981\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n",
      "File \u001b[0;32m~/learning/zindi_challenge/lacuna_chl/lacuna-malaria-detection-challenge/.venv/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py:1025\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1025\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/learning/zindi_challenge/lacuna_chl/lacuna-malaria-detection-challenge/.venv/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:205\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 205\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n",
      "File \u001b[0;32m~/learning/zindi_challenge/lacuna_chl/lacuna-malaria-detection-challenge/.venv/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:363\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/learning/zindi_challenge/lacuna_chl/lacuna-malaria-detection-challenge/.venv/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py:140\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end(data_fetcher)\n",
      "File \u001b[0;32m~/learning/zindi_challenge/lacuna_chl/lacuna-malaria-detection-challenge/.venv/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py:250\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mautomatic_optimization:\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[0;32m--> 250\u001b[0m     batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautomatic_optimization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/learning/zindi_challenge/lacuna_chl/lacuna-malaria-detection-challenge/.venv/lib/python3.9/site-packages/lightning/pytorch/loops/optimization/automatic.py:190\u001b[0m, in \u001b[0;36m_AutomaticOptimization.run\u001b[0;34m(self, optimizer, batch_idx, kwargs)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m result \u001b[38;5;241m=\u001b[39m closure\u001b[38;5;241m.\u001b[39mconsume_result()\n",
      "File \u001b[0;32m~/learning/zindi_challenge/lacuna_chl/lacuna-malaria-detection-challenge/.venv/lib/python3.9/site-packages/lightning/pytorch/loops/optimization/automatic.py:268\u001b[0m, in \u001b[0;36m_AutomaticOptimization._optimizer_step\u001b[0;34m(self, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[0;32m--> 268\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moptimizer_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n",
      "File \u001b[0;32m~/learning/zindi_challenge/lacuna_chl/lacuna-malaria-detection-challenge/.venv/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py:167\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 167\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n",
      "File \u001b[0;32m~/learning/zindi_challenge/lacuna_chl/lacuna-malaria-detection-challenge/.venv/lib/python3.9/site-packages/lightning/pytorch/core/module.py:1306\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[0;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[0m\n\u001b[1;32m   1282\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Override this method to adjust the default way the :class:`~lightning.pytorch.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[1;32m   1283\u001b[0m \u001b[38;5;124;03mthe optimizer.\u001b[39;00m\n\u001b[1;32m   1284\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1304\u001b[0m \n\u001b[1;32m   1305\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1306\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/learning/zindi_challenge/lacuna_chl/lacuna-malaria-detection-challenge/.venv/lib/python3.9/site-packages/lightning/pytorch/core/optimizer.py:153\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[0;34m(self, closure, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_after_step()\n",
      "File \u001b[0;32m~/learning/zindi_challenge/lacuna_chl/lacuna-malaria-detection-challenge/.venv/lib/python3.9/site-packages/lightning/pytorch/strategies/strategy.py:238\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[0;34m(self, optimizer, closure, model, **kwargs)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl\u001b[38;5;241m.\u001b[39mLightningModule)\n\u001b[0;32m--> 238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision_plugin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/learning/zindi_challenge/lacuna_chl/lacuna-malaria-detection-challenge/.venv/lib/python3.9/site-packages/lightning/pytorch/plugins/precision/amp.py:94\u001b[0m, in \u001b[0;36mMixedPrecision.optimizer_step\u001b[0;34m(self, optimizer, model, closure, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m skip_unscaling:\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;66;03m# note: the scaler will skip the `optimizer.step` if nonfinite gradients are found\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m     step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mupdate()\n",
      "File \u001b[0;32m~/learning/zindi_challenge/lacuna_chl/lacuna-malaria-detection-challenge/.venv/lib/python3.9/site-packages/torch/amp/grad_scaler.py:457\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    455\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 457\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_opt_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mSTEPPED\n",
      "File \u001b[0;32m~/learning/zindi_challenge/lacuna_chl/lacuna-malaria-detection-challenge/.venv/lib/python3.9/site-packages/torch/amp/grad_scaler.py:351\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf_per_device\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    352\u001b[0m     retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/learning/zindi_challenge/lacuna_chl/lacuna-malaria-detection-challenge/.venv/lib/python3.9/site-packages/torch/amp/grad_scaler.py:351\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    350\u001b[0m retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    352\u001b[0m     retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m FaceDetectionModel()\n\u001b[1;32m      3\u001b[0m trainer \u001b[38;5;241m=\u001b[39m L\u001b[38;5;241m.\u001b[39mTrainer(max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, precision\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m16-mixed\u001b[39m\u001b[38;5;124m'\u001b[39m, log_every_n_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/learning/zindi_challenge/lacuna_chl/lacuna-malaria-detection-challenge/.venv/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py:538\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 538\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/learning/zindi_challenge/lacuna_chl/lacuna-malaria-detection-challenge/.venv/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py:64\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(launcher, _SubprocessScriptLauncher):\n\u001b[1;32m     63\u001b[0m         launcher\u001b[38;5;241m.\u001b[39mkill(_get_sigkill_signal())\n\u001b[0;32m---> 64\u001b[0m     \u001b[43mexit\u001b[49m(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[1;32m     67\u001b[0m     _interrupt(trainer, exception)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "data = FacesData()\n",
    "model = FaceDetectionModel()\n",
    "\n",
    "wandb_logger = WandbLogger(log_model=\"none\")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=5, precision=\"16-mixed\", log_every_n_steps=50, logger=wandb_logger\n",
    ")\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FaceDetectionModel(\n",
       "  (model): FasterRCNN(\n",
       "    (transform): GeneralizedRCNNTransform(\n",
       "        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "        Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "    )\n",
       "    (backbone): BackboneWithFPN(\n",
       "      (body): IntermediateLayerGetter(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (1): FrozenBatchNorm2d(16, eps=1e-05)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): InvertedResidual(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
       "              (1): FrozenBatchNorm2d(16, eps=1e-05)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): FrozenBatchNorm2d(16, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): InvertedResidual(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
       "              (1): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2dNormActivation(\n",
       "              (0): Conv2d(64, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): FrozenBatchNorm2d(24, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): InvertedResidual(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): FrozenBatchNorm2d(72, eps=1e-05)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)\n",
       "              (1): FrozenBatchNorm2d(72, eps=1e-05)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2dNormActivation(\n",
       "              (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): FrozenBatchNorm2d(24, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): InvertedResidual(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): FrozenBatchNorm2d(72, eps=1e-05)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(72, 72, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=72, bias=False)\n",
       "              (1): FrozenBatchNorm2d(72, eps=1e-05)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): ReLU()\n",
       "              (scale_activation): Hardsigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(72, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): FrozenBatchNorm2d(40, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (5): InvertedResidual(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): FrozenBatchNorm2d(120, eps=1e-05)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
       "              (1): FrozenBatchNorm2d(120, eps=1e-05)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): ReLU()\n",
       "              (scale_activation): Hardsigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): FrozenBatchNorm2d(40, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (6): InvertedResidual(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): FrozenBatchNorm2d(120, eps=1e-05)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
       "              (1): FrozenBatchNorm2d(120, eps=1e-05)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): ReLU()\n",
       "              (scale_activation): Hardsigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): FrozenBatchNorm2d(40, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (7): InvertedResidual(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): FrozenBatchNorm2d(240, eps=1e-05)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
       "              (1): FrozenBatchNorm2d(240, eps=1e-05)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (2): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): FrozenBatchNorm2d(80, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (8): InvertedResidual(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 200, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): FrozenBatchNorm2d(200, eps=1e-05)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=200, bias=False)\n",
       "              (1): FrozenBatchNorm2d(200, eps=1e-05)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (2): Conv2dNormActivation(\n",
       "              (0): Conv2d(200, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): FrozenBatchNorm2d(80, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (9): InvertedResidual(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): FrozenBatchNorm2d(184, eps=1e-05)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
       "              (1): FrozenBatchNorm2d(184, eps=1e-05)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (2): Conv2dNormActivation(\n",
       "              (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): FrozenBatchNorm2d(80, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (10): InvertedResidual(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): FrozenBatchNorm2d(184, eps=1e-05)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
       "              (1): FrozenBatchNorm2d(184, eps=1e-05)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (2): Conv2dNormActivation(\n",
       "              (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): FrozenBatchNorm2d(80, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (11): InvertedResidual(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): FrozenBatchNorm2d(480, eps=1e-05)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "              (1): FrozenBatchNorm2d(480, eps=1e-05)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(480, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(120, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): ReLU()\n",
       "              (scale_activation): Hardsigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): FrozenBatchNorm2d(112, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (12): InvertedResidual(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): FrozenBatchNorm2d(672, eps=1e-05)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n",
       "              (1): FrozenBatchNorm2d(672, eps=1e-05)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): ReLU()\n",
       "              (scale_activation): Hardsigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): FrozenBatchNorm2d(112, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (13): InvertedResidual(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): FrozenBatchNorm2d(672, eps=1e-05)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
       "              (1): FrozenBatchNorm2d(672, eps=1e-05)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): ReLU()\n",
       "              (scale_activation): Hardsigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): FrozenBatchNorm2d(160, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (14): InvertedResidual(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): FrozenBatchNorm2d(960, eps=1e-05)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
       "              (1): FrozenBatchNorm2d(960, eps=1e-05)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): ReLU()\n",
       "              (scale_activation): Hardsigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): FrozenBatchNorm2d(160, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (15): InvertedResidual(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): FrozenBatchNorm2d(960, eps=1e-05)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
       "              (1): FrozenBatchNorm2d(960, eps=1e-05)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): ReLU()\n",
       "              (scale_activation): Hardsigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): FrozenBatchNorm2d(160, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (16): Conv2dNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): FrozenBatchNorm2d(960, eps=1e-05)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "      )\n",
       "      (fpn): FeaturePyramidNetwork(\n",
       "        (inner_blocks): ModuleList(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(160, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (layer_blocks): ModuleList(\n",
       "          (0-1): 2 x Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (extra_blocks): LastLevelMaxPool()\n",
       "      )\n",
       "    )\n",
       "    (rpn): RegionProposalNetwork(\n",
       "      (anchor_generator): AnchorGenerator()\n",
       "      (head): RPNHead(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (1): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (cls_logits): Conv2d(256, 15, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bbox_pred): Conv2d(256, 60, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (roi_heads): RoIHeads(\n",
       "      (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "      (box_head): TwoMLPHead(\n",
       "        (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "        (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (box_predictor): FastRCNNPredictor(\n",
       "        (cls_score): Linear(in_features=1024, out_features=91, bias=True)\n",
       "        (bbox_pred): Linear(in_features=1024, out_features=364, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
