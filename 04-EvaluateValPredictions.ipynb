{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_PTH = \"zindi_data/ValDataset.csv\"\n",
    "PRDICTION_PTH = 'zindi_data/validation/prediction_cond-detr-50_THR0.000_IOU0.400_ID580.csv'\n",
    "THR_INF = 0.01\n",
    "from zindi_code import CLS_MAPPER\n",
    "CLS_MAPPER[\"NEG\"] = 2\n",
    "CLS_MAPPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = pd.read_csv(VALID_PTH)\n",
    "\n",
    "predictions = pd.read_csv(PRDICTION_PTH)\n",
    "predictions.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid_data = valid_data.rename(columns={\n",
    "# \t\"Image_ID\": \"image_id\", \"class\": \"category_id\", \"confidence\": \"score\", \"xmin\": \"x\", \"ymin\": \"y\"\n",
    "# })\n",
    "\n",
    "# valid_data[\"w\"] = valid_data[\"xmax\"] - valid_data[\"x\"]\n",
    "# valid_data[\"h\"] = valid_data[\"ymax\"] - valid_data[\"y\"]\n",
    "\n",
    "\n",
    "# valid_data.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[\"Image_ID\"].nunique(), valid_data[\"Image_ID\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(predictions[\"Image_ID\"].unique()).intersection(valid_data[\"Image_ID\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[\"Image_ID\"].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data[\"Image_ID\"].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[\"confidence\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(valid_data[valid_data[\"class\"] == \"NEG\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "image_files: list[str] = valid_data[\"Image_ID\"].unique().tolist()\n",
    "\n",
    "\n",
    "def get_format_pred(file_id: str, data: pd.DataFrame, is_prediction, thr=THR_INF):\n",
    "    pred = data[data[\"Image_ID\"] == file_id].copy()\n",
    "    pred = pred[pred[\"class\"] != \"NEG\"]\n",
    "    pred = pred[pred[\"confidence\"] >= thr]\n",
    "    if not len(pred):\n",
    "        return dict(\n",
    "        \tboxes=torch.empty(size=[0, 4], dtype=torch.float).cuda(),\n",
    "        \tscores=torch.empty(size=[0], dtype=torch.float).cuda(),\n",
    "        \tlabels=torch.empty(size=[0,], dtype=torch.int).cuda(),\n",
    "        )\n",
    "        return dict(\n",
    "            boxes=torch.tensor([[0.0, 0.0, 0.0, 0.0]], dtype=torch.float).cuda(),\n",
    "            scores=torch.tensor([1.0], dtype=torch.float).cuda(),\n",
    "            labels=torch.tensor([CLS_MAPPER[\"NEG\"]], dtype=torch.int).cuda(),\n",
    "        )\n",
    "    pred[\"w\"] = pred[\"xmax\"] - pred[\"xmin\"]\n",
    "    pred[\"h\"] = pred[\"ymax\"] - pred[\"ymin\"]\n",
    "    boxes = torch.tensor(\n",
    "        pred[[\"xmin\", \"ymin\", \"w\", \"h\"]].values, dtype=torch.float\n",
    "    ).cuda()\n",
    "\n",
    "    scores = torch.tensor(pred[\"confidence\"].values, dtype=torch.float).cuda()\n",
    "    labels = torch.tensor(\n",
    "        [CLS_MAPPER[i] for i in pred[\"class\"].values], dtype=torch.int\n",
    "    ).cuda()\n",
    "    result = dict(boxes=boxes, scores=scores, labels=labels)\n",
    "    return result\n",
    "\n",
    "\n",
    "print(\"Prediction Extraction\")\n",
    "preds = [get_format_pred(i, predictions, True) for i in image_files]\n",
    "print(\"Expected Extraction\")\n",
    "expected = [get_format_pred(i, valid_data, False) for i in image_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.detection import MeanAveragePrecision\n",
    "\n",
    "metric = MeanAveragePrecision(\n",
    "    iou_type=\"bbox\",\n",
    "    box_format=\"xywh\",\n",
    "    class_metrics=True,\n",
    "    extended_summary=False,\n",
    "    # backend=\"faster_coco_eval\",\n",
    ")\n",
    "\n",
    "\n",
    "metric.update(preds=preds, target=expected)\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(metric.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected[128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds[128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "thrs = np.linspace(0.0, 0.95, num=15)\n",
    "best_score = 0\n",
    "best_thr = 0\n",
    "\n",
    "for thr in tqdm(thrs):\n",
    "    metric = MeanAveragePrecision(\n",
    "        iou_type=\"bbox\",\n",
    "        box_format=\"xywh\",\n",
    "        class_metrics=True,\n",
    "        extended_summary=False,\n",
    "        # backend=\"faster_coco_eval\",\n",
    "    )\n",
    "    preds = [get_format_pred(i, predictions, True, thr) for i in image_files]\n",
    "    expected = [get_format_pred(i, valid_data, False, thr) for i in image_files]\n",
    "    metric.update(preds=preds, target=expected)\n",
    "    scores = metric.compute()\n",
    "    if scores[\"map_50\"] > best_score:\n",
    "        best_score = scores[\"map_50\"]\n",
    "        best_thr = thr\n",
    "        print(best_thr, best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_thr, best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thr = 0.05\n",
    "\n",
    "metric = MeanAveragePrecision(\n",
    "    iou_type=\"bbox\",\n",
    "    box_format=\"xywh\",\n",
    "    class_metrics=True,\n",
    "    extended_summary=False,\n",
    "    backend=\"faster_coco_eval\",\n",
    ")\n",
    "preds = [get_format_pred(i, predictions, True, thr) for i in image_files]\n",
    "expected = [get_format_pred(i, valid_data, False, thr) for i in image_files]\n",
    "metric.update(preds=preds, target=expected)\n",
    "scores = metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
